{
  "projectName": "Brain Dump",
  "projectPath": "/Users/salman.rana/code/brain-dump",
  "testingRequirements": [
    "Tests must validate user-facing behavior, not implementation details",
    "Focus on what users actually do - integration tests over unit tests",
    "Don't mock excessively - test real behavior where possible",
    "Coverage metrics are meaningless - user flow coverage is everything"
  ],
  "userStories": [
    {
      "id": "23825bf6-bda0-45df-9bd0-b1952864dd72",
      "title": "Add database schema for workflow state and review findings",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "All tables created with proper types and constraints",
        "Foreign key relationships established",
        "Indexes on ticket_id, epic_id columns",
        "Migration file generated via `pnpm db:generate`",
        "Migration runs successfully via `pnpm db:migrate`",
        "Schema types exported from `src/lib/schema.ts`"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Database Schema section)",
        "src/lib/schema.ts"
      ],
      "description": "Create the database tables needed for the Universal Quality Workflow.\n\n## Tables to Create\n\n### `ticket_workflow_state`\n- `id` (text, primary key)\n- `ticket_id` (text, foreign key to tickets)\n- `current_phase` (text: 'implementation', 'ai_review', 'human_review', 'done')\n- `review_iteration` (integer, default 0)\n- `findings_count` (integer, default 0)\n- `findings_fixed` (integer, default 0)\n- `demo_generated` (boolean, default false)\n- `created_at`, `updated_at` (timestamps)\n\n### `epic_workflow_state`\n- `id` (text, primary key)\n- `epic_id` (text, foreign key to epics)\n- `tickets_total` (integer)\n- `tickets_done` (integer)\n- `current_ticket_id` (text, nullable)\n- `learnings` (text, JSON array of learnings)\n- `created_at`, `updated_at` (timestamps)\n\n### `review_findings`\n- `id` (text, primary key)\n- `ticket_id` (text, foreign key)\n- `iteration` (integer)\n- `agent` (text: 'code-reviewer', 'silent-failure-hunter', 'code-simplifier')\n- `severity` (text: 'critical', 'major', 'minor', 'suggestion')\n- `category` (text)\n- `description` (text)\n- `file_path` (text, nullable)\n- `line_number` (integer, nullable)\n- `suggested_fix` (text, nullable)\n- `status` (text: 'open', 'fixed', 'wont_fix', 'duplicate')\n- `fixed_at` (timestamp, nullable)\n- `created_at` (timestamp)\n\n### `demo_scripts`\n- `id` (text, primary key)\n- `ticket_id` (text, foreign key)\n- `steps` (text, JSON array of demo steps)\n- `generated_at` (timestamp)\n- `completed_at` (timestamp, nullable)\n- `feedback` (text, nullable)\n- `passed` (boolean, nullable)\n\n## Acceptance Criteria\n- [ ] All tables created with proper types and constraints\n- [ ] Foreign key relationships established\n- [ ] Indexes on ticket_id, epic_id columns\n- [ ] Migration file generated via `pnpm db:generate`\n- [ ] Migration runs successfully via `pnpm db:migrate`\n- [ ] Schema types exported from `src/lib/schema.ts`\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Database Schema section)",
      "priority": "high",
      "tags": [
        "database",
        "schema",
        "foundation"
      ]
    },
    {
      "id": "0dce907d-5b78-4531-84cc-c18a9405ee4b",
      "title": "Remove deprecated 'review' status from codebase",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "No references to 'review' status in constants",
        "Migration converts existing 'review' tickets to 'ai_review'",
        "TypeScript types updated (if status is typed)",
        "UI still renders all columns correctly",
        "`pnpm type-check` passes",
        "`pnpm test` passes"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Status Cleanup section)",
        "src/lib/constants.ts",
        "src/lib/utils.ts"
      ],
      "description": "The old 'review' status is being replaced by 'ai_review' and 'human_review'. Remove all references to the deprecated status.\n\n## Changes Required\n\n### `src/lib/constants.ts`\n- Remove `review` from `STATUS_OPTIONS`\n- Remove `review` from `COLUMN_STATUSES`\n- Remove `review` from `STATUS_ORDER`\n- Remove `review` from `STATUS_BADGE_CONFIG`\n\n### `src/lib/utils.ts` or wherever `getStatusColor` lives\n- Remove `review` case from switch statement\n\n### Database Migration\n```sql\nUPDATE tickets SET status = 'ai_review' WHERE status = 'review';\n```\n\n## Acceptance Criteria\n- [ ] No references to 'review' status in constants\n- [ ] Migration converts existing 'review' tickets to 'ai_review'\n- [ ] TypeScript types updated (if status is typed)\n- [ ] UI still renders all columns correctly\n- [ ] `pnpm type-check` passes\n- [ ] `pnpm test` passes\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Status Cleanup section)",
      "priority": "high",
      "tags": [
        "cleanup",
        "breaking-change"
      ]
    },
    {
      "id": "0b21cc39-c87b-4361-8142-a0e9882bb48b",
      "title": "Add MCP tools for review findings management",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "All tools have Zod input validation",
        "Proper error handling with informative messages",
        "Progress comments created for submit and fix actions",
        "Workflow state updated atomically",
        "Tools return structured MCP responses",
        "`pnpm test` includes tool tests"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (MCP Tools section)",
        "Existing tools: mcp-server/tools/"
      ],
      "description": "Create MCP tools to submit, query, and manage review findings from AI review agents.\n\n## Tools to Implement\n\n### `submit_review_finding`\nSubmit a finding from a review agent.\n\n**Input:**\n```typescript\n{\n  ticketId: string;\n  agent: 'code-reviewer' | 'silent-failure-hunter' | 'code-simplifier';\n  severity: 'critical' | 'major' | 'minor' | 'suggestion';\n  category: string;\n  description: string;\n  filePath?: string;\n  lineNumber?: number;\n  suggestedFix?: string;\n}\n```\n\n**Behavior:**\n1. Validate ticket exists and is in `ai_review` status\n2. Get or create workflow state, increment `review_iteration` if first finding of session\n3. Insert finding into `review_findings` table\n4. Increment `findings_count` in workflow state\n5. Create progress comment with finding summary\n6. Return finding ID and current counts\n\n### `mark_finding_fixed`\nMark a finding as fixed (or won't fix).\n\n**Input:**\n```typescript\n{\n  findingId: string;\n  status: 'fixed' | 'wont_fix' | 'duplicate';\n  fixDescription?: string;\n}\n```\n\n**Behavior:**\n1. Update finding status and `fixed_at` timestamp\n2. If 'fixed', increment `findings_fixed` in workflow state\n3. Create progress comment with fix description\n4. Return updated finding\n\n### `get_review_findings`\nGet findings for a ticket, optionally filtered.\n\n**Input:**\n```typescript\n{\n  ticketId: string;\n  status?: 'open' | 'fixed' | 'wont_fix' | 'duplicate';\n  severity?: 'critical' | 'major' | 'minor' | 'suggestion';\n  agent?: string;\n}\n```\n\n### `check_review_complete`\nCheck if all critical/major findings are resolved.\n\n**Input:** `{ ticketId: string }`\n\n**Returns:**\n```typescript\n{\n  complete: boolean;\n  openCritical: number;\n  openMajor: number;\n  openMinor: number;\n  totalFixed: number;\n  canProceedToHumanReview: boolean; // true if no open critical/major\n}\n```\n\n## Acceptance Criteria\n- [ ] All tools have Zod input validation\n- [ ] Proper error handling with informative messages\n- [ ] Progress comments created for submit and fix actions\n- [ ] Workflow state updated atomically\n- [ ] Tools return structured MCP responses\n- [ ] `pnpm test` includes tool tests\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (MCP Tools section)\n- Existing tools: mcp-server/tools/",
      "priority": "high",
      "tags": [
        "mcp",
        "review",
        "tools"
      ]
    },
    {
      "id": "dd0fae07-5fa0-4930-ab66-7ffa2dd4279f",
      "title": "Add MCP tools for demo script generation and feedback",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "Demo script generation validates prerequisites",
        "Steps stored as JSON array in database",
        "Status transition to `human_review` is atomic",
        "Feedback creates appropriate comments",
        "Pass/fail properly updates ticket status",
        "UI can fetch and display demo scripts"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Demo Scripts section)"
      ],
      "description": "Create MCP tools to generate demo scripts and capture human feedback during review.\n\n## Tools to Implement\n\n### `generate_demo_script`\nGenerate a demo script for human review.\n\n**Input:**\n```typescript\n{\n  ticketId: string;\n  steps: Array<{\n    order: number;\n    description: string;\n    expectedOutcome: string;\n    type: 'manual' | 'visual' | 'automated';\n  }>;\n}\n```\n\n**Behavior:**\n1. Validate ticket is in `ai_review` and all critical/major findings fixed\n2. Create demo_script record with steps\n3. Update workflow state: `demo_generated = true`\n4. Transition ticket status to `human_review`\n5. Create progress comment with step count\n6. Return demo script ID\n\n### `get_demo_script`\nGet the demo script for a ticket.\n\n**Input:** `{ ticketId: string }`\n\n**Returns:** Demo script with steps and any existing feedback\n\n### `update_demo_step`\nUpdate a single demo step's status during human review.\n\n**Input:**\n```typescript\n{\n  demoScriptId: string;\n  stepOrder: number;\n  status: 'pending' | 'passed' | 'failed' | 'skipped';\n  notes?: string;\n}\n```\n\n### `submit_demo_feedback`\nSubmit final demo feedback from human reviewer.\n\n**Input:**\n```typescript\n{\n  ticketId: string;\n  passed: boolean;\n  feedback: string;\n  stepResults?: Array<{ order: number; passed: boolean; notes?: string }>;\n}\n```\n\n**Behavior:**\n1. Update demo_script with feedback and completion timestamp\n2. If passed:\n   - Set ticket status to `done`\n   - Update workflow state\n   - Create success comment\n3. If failed:\n   - Keep ticket in `human_review` (or move back to `in_progress`)\n   - Create comment with issues for AI to address\n4. Log telemetry event\n\n## Acceptance Criteria\n- [ ] Demo script generation validates prerequisites\n- [ ] Steps stored as JSON array in database\n- [ ] Status transition to `human_review` is atomic\n- [ ] Feedback creates appropriate comments\n- [ ] Pass/fail properly updates ticket status\n- [ ] UI can fetch and display demo scripts\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Demo Scripts section)",
      "priority": "high",
      "tags": [
        "mcp",
        "demo",
        "human-review"
      ]
    },
    {
      "id": "35e86e41-7ce1-47f0-9efb-341d9808027f",
      "title": "Add MCP tool for learning reconciliation",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "Learnings stored in epic workflow state",
        "Doc updates are safe (backup before write)",
        "Progress comments created for transparency",
        "Can query learnings by epic",
        "Supports CLAUDE.md, AGENTS.md, and custom files"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Reconciliation section)",
        "Dillon Mulroy workflow: learnings feed back into rules"
      ],
      "description": "Create MCP tool to reconcile learnings from completed tickets back into project documentation.\n\n## Tool to Implement\n\n### `reconcile_learnings`\nAnalyze completed ticket work and extract learnings to update project docs.\n\n**Input:**\n```typescript\n{\n  ticketId: string;\n  learnings: Array<{\n    type: 'pattern' | 'anti-pattern' | 'tool-usage' | 'workflow';\n    description: string;\n    suggestedUpdate?: {\n      file: string; // e.g., 'CLAUDE.md', 'AGENTS.md'\n      section: string;\n      content: string;\n    };\n  }>;\n  updateDocs?: boolean; // If true, actually apply suggested updates\n}\n```\n\n**Behavior:**\n1. Validate ticket is in `done` status\n2. Store learnings in `epic_workflow_state.learnings` JSON array\n3. If `updateDocs` is true:\n   - Read target file\n   - Find or create section\n   - Append learning content\n   - Write file back\n4. Create progress comment listing updates made\n5. Return summary of learnings and any doc updates\n\n### `get_epic_learnings`\nGet accumulated learnings for an epic.\n\n**Input:** `{ epicId: string }`\n\n**Returns:**\n```typescript\n{\n  epicId: string;\n  ticketsCompleted: number;\n  learnings: Array<{\n    ticketId: string;\n    ticketTitle: string;\n    learnings: Learning[];\n    appliedAt?: string;\n  }>;\n}\n```\n\n## Acceptance Criteria\n- [ ] Learnings stored in epic workflow state\n- [ ] Doc updates are safe (backup before write)\n- [ ] Progress comments created for transparency\n- [ ] Can query learnings by epic\n- [ ] Supports CLAUDE.md, AGENTS.md, and custom files\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Reconciliation section)\n- Dillon Mulroy workflow: learnings feed back into rules",
      "priority": "medium",
      "tags": [
        "mcp",
        "learnings",
        "documentation"
      ]
    },
    {
      "id": "85117900-152e-49b5-b284-bab6b1169520",
      "title": "Create Claude Code telemetry hooks",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "All 6 hooks created and executable",
        "Queue file pattern implemented with JSONL format",
        "Correlation IDs properly pair start/end events",
        "Hooks merge with existing settings (not overwrite)",
        "Works with `claude --print-hooks` for debugging",
        "Error handling doesn't break Claude session",
        "Documentation added to CLAUDE.md"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Claude Code Telemetry section)",
        "Claude Code Hooks: https://code.claude.com/docs/en/hooks"
      ],
      "description": "Implement the full set of Claude Code hooks for telemetry capture and workflow enforcement.\n\n## Hooks to Create\n\n### Session Lifecycle\n1. **`start-telemetry-session.sh`** (SessionStart hook)\n   - Detect active ticket from `.claude/ralph-state.json`\n   - Output message for Claude to call `start_telemetry_session`\n   - Create `.claude/telemetry-session.json` with session ID\n\n2. **`end-telemetry-session.sh`** (Stop hook)\n   - Read session ID from telemetry session file\n   - Flush remaining events from queue\n   - Output message for Claude to call `end_telemetry_session`\n   - Clean up session file\n\n### Tool Events\n3. **`log-tool-start.sh`** (PreToolUse hook)\n   - Generate correlation ID\n   - Store in `.claude/tool-correlations.json`\n   - Write start event to `.claude/telemetry-queue.jsonl`\n\n4. **`log-tool-end.sh`** (PostToolUse hook)\n   - Find correlation ID from correlations file\n   - Write end event with success=true to queue\n\n5. **`log-tool-failure.sh`** (PostToolUseFailure hook)\n   - Find correlation ID\n   - Write end event with success=false and error message to queue\n\n### Prompt Capture\n6. **`log-prompt.sh`** (UserPromptSubmit hook)\n   - Read session ID\n   - Output message for Claude to call `log_prompt_event`\n\n## Configuration\n\nUpdate `~/.claude/settings.json`:\n```json\n{\n  \"hooks\": {\n    \"SessionStart\": [{ \"type\": \"command\", \"command\": \"~/.claude/hooks/start-telemetry-session.sh\" }],\n    \"Stop\": [{ \"type\": \"command\", \"command\": \"~/.claude/hooks/end-telemetry-session.sh\" }],\n    \"PreToolUse\": [{ \"type\": \"command\", \"command\": \"~/.claude/hooks/log-tool-start.sh\" }],\n    \"PostToolUse\": [{ \"type\": \"command\", \"command\": \"~/.claude/hooks/log-tool-end.sh\" }],\n    \"PostToolUseFailure\": [{ \"type\": \"command\", \"command\": \"~/.claude/hooks/log-tool-failure.sh\" }],\n    \"UserPromptSubmit\": [{ \"type\": \"command\", \"command\": \"~/.claude/hooks/log-prompt.sh\" }]\n  }\n}\n```\n\n## File Patterns\n- Queue file: `.claude/telemetry-queue.jsonl`\n- Correlations: `.claude/tool-correlations.json`\n- Session: `.claude/telemetry-session.json`\n\n## Acceptance Criteria\n- [ ] All 6 hooks created and executable\n- [ ] Queue file pattern implemented with JSONL format\n- [ ] Correlation IDs properly pair start/end events\n- [ ] Hooks merge with existing settings (not overwrite)\n- [ ] Works with `claude --print-hooks` for debugging\n- [ ] Error handling doesn't break Claude session\n- [ ] Documentation added to CLAUDE.md\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Claude Code Telemetry section)\n- Claude Code Hooks: https://code.claude.com/docs/en/hooks",
      "priority": "high",
      "tags": [
        "claude-code",
        "hooks",
        "telemetry"
      ]
    },
    {
      "id": "f4d0ca6a-5522-4c33-8f2e-c4e3f22196b7",
      "title": "Create Cursor telemetry hooks",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "All 6 hooks created for Cursor format",
        "`.cursor/hooks.json` template created",
        "Hooks use `CURSOR_PROJECT_DIR` correctly",
        "Test compatibility with Claude Code hooks",
        "Exit code 2 blocks actions properly",
        "Document differences from Claude Code hooks"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Cursor section)",
        "Cursor Hooks: https://cursor.com/docs/agent/hooks",
        "Third-Party Hooks: https://cursor.com/docs/agent/third-party-hooks"
      ],
      "description": "Implement Cursor-specific hooks for telemetry capture. Cursor has full hook support similar to Claude Code.\n\n## Hooks to Create\n\nCursor hooks go in `.cursor/hooks/` or `~/.cursor/hooks/` (global).\n\n### Session Lifecycle\n1. **`start-telemetry.sh`** (sessionStart hook)\n   - Same logic as Claude Code version\n   - Uses `CURSOR_PROJECT_DIR` env var\n\n2. **`end-telemetry.sh`** (sessionEnd hook)\n   - Flush queue and end session\n\n### Tool Events\n3. **`log-tool-start.sh`** (preToolUse hook)\n4. **`log-tool-end.sh`** (postToolUse hook)\n5. **`log-tool-failure.sh`** (postToolUseFailure hook)\n\n### Prompt Capture\n6. **`log-prompt.sh`** (beforeSubmitPrompt hook)\n\n## Configuration\n\nCreate `.cursor/hooks.json`:\n```json\n{\n  \"version\": 1,\n  \"hooks\": {\n    \"sessionStart\": [{ \"command\": \"~/.cursor/hooks/start-telemetry.sh\" }],\n    \"sessionEnd\": [{ \"command\": \"~/.cursor/hooks/end-telemetry.sh\" }],\n    \"preToolUse\": [{ \"command\": \"~/.cursor/hooks/log-tool-start.sh\" }],\n    \"postToolUse\": [{ \"command\": \"~/.cursor/hooks/log-tool-end.sh\" }],\n    \"postToolUseFailure\": [{ \"command\": \"~/.cursor/hooks/log-tool-failure.sh\" }],\n    \"beforeSubmitPrompt\": [{ \"command\": \"~/.cursor/hooks/log-prompt.sh\" }]\n  }\n}\n```\n\n## Claude Code Hook Compatibility\n\nCursor can load Claude Code hooks from `.claude/settings.json`. Test that:\n1. Cursor detects existing Claude Code hooks\n2. Brain Dump hooks work in both environments\n3. No conflicts between Cursor and Claude hook formats\n\n## Acceptance Criteria\n- [ ] All 6 hooks created for Cursor format\n- [ ] `.cursor/hooks.json` template created\n- [ ] Hooks use `CURSOR_PROJECT_DIR` correctly\n- [ ] Test compatibility with Claude Code hooks\n- [ ] Exit code 2 blocks actions properly\n- [ ] Document differences from Claude Code hooks\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Cursor section)\n- Cursor Hooks: https://cursor.com/docs/agent/hooks\n- Third-Party Hooks: https://cursor.com/docs/agent/third-party-hooks",
      "priority": "medium",
      "tags": [
        "cursor",
        "hooks",
        "telemetry"
      ]
    },
    {
      "id": "077526d0-cd8d-4ada-a00b-b1d31ba2431c",
      "title": "Create OpenCode telemetry plugin",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "Plugin TypeScript file created",
        "Plugin handles all relevant lifecycle events",
        "Session start/end properly tracked",
        "Tool events logged with correlation",
        "`AGENTS.md` template created",
        "Skill created for workflow guidance",
        "MCP config snippet documented",
        "Test with `opencode mcp debug brain-dump`"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (OpenCode section)",
        "OpenCode Plugins: https://opencode.ai/docs/plugins/",
        "OpenCode MCP: https://opencode.ai/docs/mcp-servers/"
      ],
      "description": "Implement an OpenCode plugin for telemetry capture using OpenCode's 40+ lifecycle events.\n\n## Plugin Implementation\n\nCreate `~/.config/opencode/plugins/brain-dump-telemetry.ts`:\n\n```typescript\nimport type { Plugin } from \"@opencode-ai/plugin\"\n\nconst toolCorrelations = new Map<string, { startTime: number; correlationId: string }>()\n\nexport const BrainDumpTelemetry: Plugin = async ({ client, project }) => {\n  let sessionId: string | null = null\n\n  return {\n    // Session lifecycle\n    \"session.created\": async (input) => {\n      const result = await client.callTool(\"mcp__brain-dump__start_telemetry_session\", {\n        projectPath: project?.path,\n      })\n      sessionId = result?.sessionId\n    },\n\n    \"session.idle\": async (input) => {\n      if (sessionId) {\n        await client.callTool(\"mcp__brain-dump__end_telemetry_session\", {\n          sessionId,\n          outcome: \"success\",\n        })\n        sessionId = null\n      }\n    },\n\n    \"session.error\": async (input) => {\n      if (sessionId) {\n        await client.callTool(\"mcp__brain-dump__end_telemetry_session\", {\n          sessionId,\n          outcome: \"failure\",\n        })\n        sessionId = null\n      }\n    },\n\n    // Tool lifecycle\n    \"tool.execute.before\": async (input, output) => {\n      if (!sessionId) return\n      \n      const correlationId = crypto.randomUUID()\n      toolCorrelations.set(input.tool, { startTime: Date.now(), correlationId })\n      \n      await client.callTool(\"mcp__brain-dump__log_tool_event\", {\n        sessionId,\n        event: \"start\",\n        toolName: input.tool,\n        correlationId,\n        params: { /* sanitized params */ },\n      })\n    },\n\n    \"tool.execute.after\": async (input, output) => {\n      if (!sessionId) return\n      \n      const correlation = toolCorrelations.get(input.tool)\n      if (!correlation) return\n      \n      const duration = Date.now() - correlation.startTime\n      toolCorrelations.delete(input.tool)\n      \n      await client.callTool(\"mcp__brain-dump__log_tool_event\", {\n        sessionId,\n        event: \"end\",\n        toolName: input.tool,\n        correlationId: correlation.correlationId,\n        success: !output.error,\n        durationMs: duration,\n        error: output.error?.message,\n      })\n    },\n  }\n}\n\nexport default BrainDumpTelemetry\n```\n\n## Supporting Files\n\n### AGENTS.md Template\nCreate template for workflow enforcement via rules:\n```markdown\n# Brain Dump Workflow\n\n## Before Starting Work\nAlways call `start_ticket_work` before writing code.\n\n## After Completing Work\nAlways call `complete_ticket_work` to trigger AI review.\n...\n```\n\n### Skill\nCreate `.opencode/skills/brain-dump-workflow/SKILL.md`\n\n### MCP Config\nAdd to `opencode.json`:\n```json\n{\n  \"mcp\": {\n    \"brain-dump\": {\n      \"type\": \"local\",\n      \"command\": \"node\",\n      \"args\": [\"/path/to/brain-dump/mcp-server/index.js\"]\n    }\n  }\n}\n```\n\n## Acceptance Criteria\n- [ ] Plugin TypeScript file created\n- [ ] Plugin handles all relevant lifecycle events\n- [ ] Session start/end properly tracked\n- [ ] Tool events logged with correlation\n- [ ] `AGENTS.md` template created\n- [ ] Skill created for workflow guidance\n- [ ] MCP config snippet documented\n- [ ] Test with `opencode mcp debug brain-dump`\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (OpenCode section)\n- OpenCode Plugins: https://opencode.ai/docs/plugins/\n- OpenCode MCP: https://opencode.ai/docs/mcp-servers/",
      "priority": "medium",
      "tags": [
        "opencode",
        "plugin",
        "telemetry"
      ]
    },
    {
      "id": "a9e64194-e632-409a-b315-09c8cfd78ef7",
      "title": "Create VS Code integration templates",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "`.vscode/mcp.json` template created",
        "`.github/copilot-instructions.md` template created",
        "`.github/skills/brain-dump-workflow.skill.md` created",
        "MCP server works with VS Code's stdio transport",
        "Document VS Code MCP server trust flow",
        "Install script offers to create these files"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (VS Code section)",
        "VS Code MCP: https://code.visualstudio.com/docs/copilot/customization/mcp-servers",
        "VS Code Instructions: https://code.visualstudio.com/docs/copilot/customization/custom-instructions"
      ],
      "description": "Create VS Code integration using instructions-based approach (no extension). VS Code doesn't have hooks, so we rely on MCP preconditions and prompt guidance.\n\n## Files to Create\n\n### MCP Configuration\n`.vscode/mcp.json`:\n```json\n{\n  \"mcpServers\": {\n    \"brain-dump\": {\n      \"command\": \"node\",\n      \"args\": [\"${workspaceFolder}/node_modules/brain-dump/mcp-server/index.js\"],\n      \"env\": {}\n    }\n  }\n}\n```\n\n### Custom Instructions\n`.github/copilot-instructions.md`:\n```markdown\n# Brain Dump Workflow Integration\n\nWhen working on tickets in this project, follow the Brain Dump workflow:\n\n## Starting Work\n1. Call `mcp__brain-dump__start_ticket_work({ ticketId })` before writing code\n2. This creates a branch and sets up tracking\n\n## During Development\n- Commit frequently with `feat(<ticket-id>): <description>` format\n- Call `mcp__brain-dump__link_commit_to_ticket` after commits\n\n## Completing Work\n1. Call `mcp__brain-dump__complete_ticket_work({ ticketId, summary })`\n2. This triggers AI review and moves ticket to ai_review status\n3. Fix any critical/major findings before proceeding\n\n## Human Review\n- AI will generate a demo script\n- Wait for human approval before marking done\n```\n\n### Agent Skill\n`.github/skills/brain-dump-workflow.skill.md`:\n```markdown\n---\nname: brain-dump-workflow\ndescription: Manages ticket workflow for Brain Dump projects\napplyTo: \"**/*\"\n---\n\n# Brain Dump Workflow Skill\n\nWhen the user asks to start work on a ticket, work on a task, or implement a feature:\n\n1. First call `mcp__brain-dump__list_tickets` to see available tickets\n2. Use `mcp__brain-dump__start_ticket_work` to begin\n...\n```\n\n## Acceptance Criteria\n- [ ] `.vscode/mcp.json` template created\n- [ ] `.github/copilot-instructions.md` template created\n- [ ] `.github/skills/brain-dump-workflow.skill.md` created\n- [ ] MCP server works with VS Code's stdio transport\n- [ ] Document VS Code MCP server trust flow\n- [ ] Install script offers to create these files\n\n## Note\nVS Code extension with Language Model Tool API is OUT OF SCOPE. We use instructions-based approach which provides medium enforcement via prompt guidance + MCP preconditions.\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (VS Code section)\n- VS Code MCP: https://code.visualstudio.com/docs/copilot/customization/mcp-servers\n- VS Code Instructions: https://code.visualstudio.com/docs/copilot/customization/custom-instructions",
      "priority": "low",
      "tags": [
        "vscode",
        "instructions",
        "mcp"
      ]
    },
    {
      "id": "099c8f58-42f9-4901-9806-cd83612b7e87",
      "title": "Create workflow skills (/next-task, /review-ticket, /demo)",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "All 5 skills created with proper frontmatter",
        "Skills have clear step-by-step instructions",
        "`allowed-tools` properly scoped",
        "Skills work via `/skill-name` invocation",
        "Documentation for each skill usage"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Skills section)",
        "Claude Code Skills: https://code.claude.com/docs/en/skills"
      ],
      "description": "Create Claude Code skills (slash commands) for the Universal Quality Workflow.\n\n## Skills to Create\n\n### `/next-task`\nIntelligently select the next ticket to work on.\n\nFile: `~/.claude/commands/next-task.md`\n\n```markdown\n---\ndescription: Select and start the next ticket using Brain Dump workflow\nallowed-tools: [mcp__brain-dump__*]\n---\n\n# Next Task Selection\n\n1. Call `list_tickets({ status: 'ready', limit: 10 })` to see available tickets\n2. Consider:\n   - Priority (high > medium > low)\n   - Dependencies (unblocked tickets first)\n   - Epic context (continue current epic if possible)\n3. Present top 3 recommendations with rationale\n4. Once user selects, call `start_ticket_work({ ticketId })`\n```\n\n### `/review-ticket`\nRun AI review on current ticket.\n\nFile: `~/.claude/commands/review-ticket.md`\n\n```markdown\n---\ndescription: Run AI review agents on current ticket work\nallowed-tools: [mcp__brain-dump__*, Task]\n---\n\n# Ticket Review\n\n1. Get current ticket from ralph-state.json\n2. Run review agents in parallel:\n   - code-reviewer\n   - silent-failure-hunter  \n   - code-simplifier\n3. Submit findings via `submit_review_finding`\n4. Summarize findings by severity\n5. If critical/major found, list specific fixes needed\n```\n\n### `/review-epic`\nRun Tracer Review on entire epic.\n\nFile: `~/.claude/commands/review-epic.md`\n\n### `/demo`\nGenerate demo script for human review.\n\nFile: `~/.claude/commands/demo.md`\n\n```markdown\n---\ndescription: Generate demo script for human review approval\nallowed-tools: [mcp__brain-dump__*]\n---\n\n# Demo Script Generation\n\n1. Get current ticket and verify all critical/major findings fixed\n2. Analyze ticket requirements and implemented changes\n3. Generate demo steps covering:\n   - Setup/prerequisites\n   - Core functionality verification\n   - Edge cases\n   - Visual confirmation points\n4. Call `generate_demo_script({ ticketId, steps })`\n5. Summarize what human should verify\n```\n\n### `/reconcile-learnings`\nReconcile learnings from completed work.\n\nFile: `~/.claude/commands/reconcile-learnings.md`\n\n## Acceptance Criteria\n- [ ] All 5 skills created with proper frontmatter\n- [ ] Skills have clear step-by-step instructions\n- [ ] `allowed-tools` properly scoped\n- [ ] Skills work via `/skill-name` invocation\n- [ ] Documentation for each skill usage\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Skills section)\n- Claude Code Skills: https://code.claude.com/docs/en/skills",
      "priority": "high",
      "tags": [
        "skills",
        "commands",
        "workflow"
      ]
    },
    {
      "id": "0cfb2286-7570-497c-ad93-a8e79c39f638",
      "title": "Build Human Review UI with demo approval flow",
      "passes": false,
      "overview": "",
      "types": [
        {
          "name": "DemoPanelProps",
          "code": "interface DemoPanelProps {\n  ticketId: string;\n  demoScript: DemoScript;\n  onApprove: (feedback: string) => void;\n  onReject: (feedback: string, failedSteps: number[]) => void;\n}"
        },
        {
          "name": "DemoStepProps",
          "code": "interface DemoStepProps {\n  step: DemoStep;\n  onStatusChange: (status: 'pending' | 'passed' | 'failed' | 'skipped') => void;\n  onNotesChange: (notes: string) => void;\n}"
        }
      ],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "DemoPanel component with all features",
        "DemoStep component with pass/fail/skip",
        "Kanban card shows \"Demo Ready\" badge",
        "Ticket detail shows demo review CTA",
        "Server functions created",
        "TanStack Query hooks created",
        "Approval triggers ticket status to `done`",
        "Rejection keeps ticket in `human_review` with feedback comment",
        "Toast notification when ticket enters `human_review`"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Human Review UI section)"
      ],
      "description": "Create the UI components for human review with demo script approval.\n\n## Components to Build\n\n### `DemoPanel.tsx`\nMain panel showing demo steps for human review.\n\n```typescript\ninterface DemoPanelProps {\n  ticketId: string;\n  demoScript: DemoScript;\n  onApprove: (feedback: string) => void;\n  onReject: (feedback: string, failedSteps: number[]) => void;\n}\n```\n\nFeatures:\n- Display all demo steps with order\n- Progress indicator showing completed steps\n- Overall feedback text area\n- \"Approve & Complete\" button (disabled until all steps marked)\n- \"Request Changes\" button (requires at least one failed step or feedback)\n\n### `DemoStep.tsx`\nIndividual demo step with pass/fail/skip controls.\n\n```typescript\ninterface DemoStepProps {\n  step: DemoStep;\n  onStatusChange: (status: 'pending' | 'passed' | 'failed' | 'skipped') => void;\n  onNotesChange: (notes: string) => void;\n}\n```\n\nFeatures:\n- Step description and expected outcome\n- Pass/Fail/Skip buttons (radio-style)\n- Optional notes field\n- Visual indicator for step type (manual, visual, automated)\n\n### Kanban Card Updates\n- Show \"Demo Ready\" badge when ticket is in `human_review`\n- Prominent visual indicator for action required\n\n### Ticket Detail Updates\n- Show \"Start Demo Review\" CTA when in `human_review`\n- Embed DemoPanel in ticket detail view\n- Show demo history for completed demos\n\n## Server Functions\n\n```typescript\n// src/api/demo.ts\nexport const getDemoScript = createServerFn({ method: 'GET' })\n  .inputValidator(z.object({ ticketId: z.string() }))\n  .handler(async ({ ticketId }) => {\n    // Fetch demo script from database\n  });\n\nexport const updateDemoStep = createServerFn({ method: 'POST' })\n  .inputValidator(z.object({\n    demoScriptId: z.string(),\n    stepOrder: z.number(),\n    status: z.enum(['pending', 'passed', 'failed', 'skipped']),\n    notes: z.string().optional(),\n  }))\n  .handler(async (input) => {\n    // Update step in database\n  });\n\nexport const submitDemoFeedback = createServerFn({ method: 'POST' })\n  .inputValidator(z.object({\n    ticketId: z.string(),\n    passed: z.boolean(),\n    feedback: z.string(),\n  }))\n  .handler(async (input) => {\n    // Submit feedback, update ticket status\n  });\n```\n\n## TanStack Query Hooks\n\n```typescript\n// src/lib/hooks.ts\nexport const useDemoScript = (ticketId: string) => {\n  return useQuery({\n    queryKey: queryKeys.demoScript(ticketId),\n    queryFn: () => getDemoScript({ ticketId }),\n    enabled: !!ticketId,\n  });\n};\n\nexport const useSubmitDemoFeedback = () => {\n  const queryClient = useQueryClient();\n  return useMutation({\n    mutationFn: submitDemoFeedback,\n    onSuccess: (_, { ticketId }) => {\n      queryClient.invalidateQueries({ queryKey: queryKeys.ticket(ticketId) });\n      queryClient.invalidateQueries({ queryKey: queryKeys.demoScript(ticketId) });\n    },\n  });\n};\n```\n\n## Acceptance Criteria\n- [ ] DemoPanel component with all features\n- [ ] DemoStep component with pass/fail/skip\n- [ ] Kanban card shows \"Demo Ready\" badge\n- [ ] Ticket detail shows demo review CTA\n- [ ] Server functions created\n- [ ] TanStack Query hooks created\n- [ ] Approval triggers ticket status to `done`\n- [ ] Rejection keeps ticket in `human_review` with feedback comment\n- [ ] Toast notification when ticket enters `human_review`\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Human Review UI section)",
      "priority": "high",
      "tags": [
        "ui",
        "react",
        "human-review"
      ]
    },
    {
      "id": "c31ad292-f515-4118-b139-0c4a41e1b4fd",
      "title": "Create multi-environment install/uninstall scripts",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "`install.sh` detects all 4 environments",
        "`install.sh` installs hooks/plugins for each detected env",
        "`install.sh` merges with existing configs (doesn't overwrite)",
        "`uninstall.sh` cleanly removes Brain Dump configs",
        "`uninstall.sh` preserves other hooks/settings",
        "`brain-dump doctor` verifies all environments",
        "`brain-dump doctor` reports missing/broken configs",
        "Scripts work on macOS and Linux",
        "Documentation for manual installation"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Installation section)",
        "Existing setup: scripts/setup-claude-code.sh"
      ],
      "description": "Create installation scripts that configure Brain Dump for all supported AI coding environments.\n\n## Scripts to Create\n\n### `scripts/install.sh`\nMain installation script that detects and configures all environments.\n\n```bash\n#!/bin/bash\n# Brain Dump Universal Quality Workflow Installer\n\necho \"ðŸ§  Brain Dump - Universal Quality Workflow Installer\"\necho \"\"\n\n# Detect environments\ndetect_claude_code() { command -v claude &>/dev/null; }\ndetect_cursor() { [ -d \"$HOME/.cursor\" ] || [ -d \"/Applications/Cursor.app\" ]; }\ndetect_opencode() { command -v opencode &>/dev/null; }\ndetect_vscode() { command -v code &>/dev/null; }\n\n# Install functions for each environment\ninstall_claude_code() {\n  echo \"ðŸ“¦ Installing Claude Code hooks...\"\n  mkdir -p ~/.claude/hooks\n  cp hooks/claude-code/*.sh ~/.claude/hooks/\n  chmod +x ~/.claude/hooks/*.sh\n  \n  # Merge hooks into settings.json (don't overwrite existing)\n  # Use jq to merge hook arrays\n}\n\ninstall_cursor() {\n  echo \"ðŸ“¦ Installing Cursor hooks...\"\n  mkdir -p ~/.cursor/hooks\n  cp hooks/cursor/*.sh ~/.cursor/hooks/\n  chmod +x ~/.cursor/hooks/*.sh\n  \n  # Create or update ~/.cursor/hooks.json\n}\n\ninstall_opencode() {\n  echo \"ðŸ“¦ Installing OpenCode plugin...\"\n  mkdir -p ~/.config/opencode/plugins\n  cp plugins/brain-dump-telemetry.ts ~/.config/opencode/plugins/\n  \n  # Create AGENTS.md template if not exists\n}\n\ninstall_vscode() {\n  echo \"ðŸ“¦ Creating VS Code templates...\"\n  # Offer to create .vscode/mcp.json\n  # Offer to create .github/copilot-instructions.md\n}\n\n# Main flow\necho \"Detecting installed environments...\"\n[ detect_claude_code ] && echo \"  âœ“ Claude Code\"\n[ detect_cursor ] && echo \"  âœ“ Cursor\"\n[ detect_opencode ] && echo \"  âœ“ OpenCode\"\n[ detect_vscode ] && echo \"  âœ“ VS Code\"\n\n# Install to each detected environment\n# ...\n```\n\n### `scripts/uninstall.sh`\nClean removal without breaking other configurations.\n\n```bash\n#!/bin/bash\n# Brain Dump Uninstaller\n\nuninstall_claude_code() {\n  echo \"ðŸ—‘ï¸  Removing Claude Code hooks...\"\n  rm -f ~/.claude/hooks/start-telemetry-session.sh\n  rm -f ~/.claude/hooks/end-telemetry-session.sh\n  rm -f ~/.claude/hooks/log-tool-*.sh\n  rm -f ~/.claude/hooks/log-prompt.sh\n  \n  # Remove hooks from settings.json (preserve other hooks)\n}\n\n# Similar for other environments...\n```\n\n### `brain-dump doctor`\nCLI command to verify all environments are configured correctly.\n\n```bash\n# Add to cli/index.ts\ndoctor() {\n  console.log(\"ðŸ©º Brain Dump Environment Doctor\")\n  \n  // Check Claude Code\n  checkClaudeCode() // hooks installed? settings.json correct?\n  \n  // Check Cursor  \n  checkCursor() // hooks.json exists? hooks installed?\n  \n  // Check OpenCode\n  checkOpenCode() // plugin installed? MCP configured?\n  \n  // Check VS Code\n  checkVSCode() // mcp.json exists? instructions exist?\n  \n  // Report summary\n}\n```\n\n## Acceptance Criteria\n- [ ] `install.sh` detects all 4 environments\n- [ ] `install.sh` installs hooks/plugins for each detected env\n- [ ] `install.sh` merges with existing configs (doesn't overwrite)\n- [ ] `uninstall.sh` cleanly removes Brain Dump configs\n- [ ] `uninstall.sh` preserves other hooks/settings\n- [ ] `brain-dump doctor` verifies all environments\n- [ ] `brain-dump doctor` reports missing/broken configs\n- [ ] Scripts work on macOS and Linux\n- [ ] Documentation for manual installation\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Installation section)\n- Existing setup: scripts/setup-claude-code.sh",
      "priority": "medium",
      "tags": [
        "scripts",
        "installation",
        "multi-environment"
      ]
    },
    {
      "id": "44fdf6df-0f14-4067-8ac8-f3b1e7a901c1",
      "title": "Update Ralph script to use new workflow",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "Ralph prompt updated with new workflow",
        "Ralph respects `human_review` status (doesn't pick these)",
        "Ralph runs AI review after implementation",
        "Ralph generates demo before stopping",
        "Ralph stops at `human_review` (doesn't auto-complete)",
        "Telemetry integrated into Ralph session",
        "Progress.txt documents workflow transitions"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md",
        "Current Ralph: scripts/ralph-epic-*.sh"
      ],
      "description": "Update the Ralph autonomous agent script to use the new Universal Quality Workflow.\n\n## Changes Required\n\n### Ralph Prompt Updates\nUpdate the prompt in `scripts/ralph-epic-*.sh` to:\n\n1. **Use new status flow**: `in_progress` â†’ `ai_review` â†’ `human_review` â†’ `done`\n\n2. **Run AI review after implementation**:\n   ```\n   After implementing and tests pass:\n   1. Call complete_ticket_work (moves to ai_review)\n   2. Run /review-ticket skill\n   3. Fix any critical/major findings\n   4. Generate demo script\n   5. Stop and wait for human review\n   ```\n\n3. **Handle human_review status**:\n   - Skip tickets in `human_review` (waiting for human)\n   - Only pick tickets in `ready` or `backlog`\n\n4. **Use telemetry**:\n   - Call `start_telemetry_session` at session start\n   - Telemetry hooks handle tool events automatically\n   - Call `end_telemetry_session` at session end\n\n### Ralph Loop Updates\nThe ralph-loop plugin should:\n- Check for `human_review` tickets and notify user\n- Not auto-continue past `ai_review` without human approval\n- Show demo approval prompts in UI\n\n## New Ralph Prompt Section\n\n```markdown\n## Workflow (Updated)\n\n1. Read plans/prd.json for incomplete tickets\n2. Skip tickets in 'human_review' (waiting for human approval)\n3. Pick ONE ticket from 'ready' or 'backlog'\n4. Call start_ticket_work(ticketId)\n5. Implement the feature (write code, run tests)\n6. Call complete_ticket_work(ticketId, summary)\n   - This moves ticket to 'ai_review'\n7. Run AI review: /review-ticket\n8. Fix any critical/major findings\n9. Generate demo: /demo\n   - This moves ticket to 'human_review'\n10. STOP - Wait for human to approve demo\n11. If all tickets complete or in human_review, output: PRD_COMPLETE\n```\n\n## Acceptance Criteria\n- [ ] Ralph prompt updated with new workflow\n- [ ] Ralph respects `human_review` status (doesn't pick these)\n- [ ] Ralph runs AI review after implementation\n- [ ] Ralph generates demo before stopping\n- [ ] Ralph stops at `human_review` (doesn't auto-complete)\n- [ ] Telemetry integrated into Ralph session\n- [ ] Progress.txt documents workflow transitions\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md\n- Current Ralph: scripts/ralph-epic-*.sh",
      "priority": "medium",
      "tags": [
        "ralph",
        "workflow",
        "automation"
      ]
    },
    {
      "id": "db3ca5cc-71a1-48e6-bf6a-efe5ce81651e",
      "title": "Add workflow telemetry dashboard to ticket detail",
      "passes": false,
      "overview": "",
      "types": [
        {
          "name": "TelemetryPanelProps",
          "code": "interface TelemetryPanelProps {\n  ticketId: string;\n}"
        }
      ],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "TelemetryPanel component shows session list",
        "Session detail shows tool breakdown",
        "Timeline view for event sequence",
        "Server function fetches and aggregates telemetry",
        "TanStack Query hook for telemetry data",
        "Integrated into ticket detail page",
        "Performance: lazy load telemetry data",
        "Empty state when no telemetry exists"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (Telemetry section)",
        "Existing telemetry MCP tools: mcp-server/tools/telemetry.js"
      ],
      "description": "Display telemetry data in the Brain Dump UI to provide observability into AI work sessions.\n\n## UI Components\n\n### `TelemetryPanel.tsx`\nShows telemetry summary for a ticket's work sessions.\n\n```typescript\ninterface TelemetryPanelProps {\n  ticketId: string;\n}\n```\n\nFeatures:\n- Session list with duration, outcome, tool counts\n- Expandable session detail showing:\n  - Tool usage breakdown (by tool name)\n  - Token usage (if available)\n  - Prompt count\n  - Error rate\n- Timeline view of tool events\n- Filter by session, tool type\n\n### `ToolUsageChart.tsx`\nVisual breakdown of tools used in a session.\n\n- Bar chart or pie chart of tool distribution\n- Color coding by tool category (Read, Write, Edit, Bash, MCP)\n- Hover for details\n\n### `SessionTimeline.tsx`\nTimeline visualization of session events.\n\n- Vertical timeline with events\n- Color coding: tool start (blue), tool end (green), error (red)\n- Expandable event details\n\n## Server Functions\n\n```typescript\n// src/api/telemetry.ts\nexport const getTicketTelemetry = createServerFn({ method: 'GET' })\n  .inputValidator(z.object({ ticketId: z.string() }))\n  .handler(async ({ ticketId }) => {\n    // Get telemetry sessions for ticket\n    // Aggregate tool usage stats\n    // Return structured data for UI\n  });\n```\n\n## Integration Points\n\n### Ticket Detail Page\n- Add \"Telemetry\" tab or collapsible section\n- Show summary stats: sessions, total tools, avg duration\n- Link to expand full telemetry view\n\n### Kanban Card\n- Small telemetry indicator (session count, last activity)\n\n## Acceptance Criteria\n- [ ] TelemetryPanel component shows session list\n- [ ] Session detail shows tool breakdown\n- [ ] Timeline view for event sequence\n- [ ] Server function fetches and aggregates telemetry\n- [ ] TanStack Query hook for telemetry data\n- [ ] Integrated into ticket detail page\n- [ ] Performance: lazy load telemetry data\n- [ ] Empty state when no telemetry exists\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (Telemetry section)\n- Existing telemetry MCP tools: mcp-server/tools/telemetry.js",
      "priority": "low",
      "tags": [
        "ui",
        "telemetry",
        "observability"
      ]
    },
    {
      "id": "877ae995-8ad9-439c-90d3-3ff6c89b49a6",
      "title": "Update documentation for Universal Quality Workflow",
      "passes": false,
      "overview": "Brain Dump implements Dillon Mulroy's \"Next-Task\" and \"Tracer Review\" workflows...",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "CLAUDE.md updated with workflow section",
        "README.md updated with features",
        "docs/universal-workflow.md created",
        "Environment-specific docs created",
        "All documentation references spec",
        "Installation instructions clear",
        "Troubleshooting section added"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (full reference)"
      ],
      "description": "Update all documentation to reflect the new Universal Quality Workflow.\n\n## Documentation Updates\n\n### CLAUDE.md\nAdd section explaining the workflow:\n- Status flow diagram\n- When AI review happens\n- When human review happens\n- How to use the skills\n- Telemetry overview\n\n### README.md\nUpdate features section:\n- Mention multi-environment support\n- Link to workflow documentation\n- Installation instructions\n\n### docs/universal-workflow.md (New)\nComprehensive documentation:\n\n```markdown\n# Universal Quality Workflow\n\n## Overview\nBrain Dump implements Dillon Mulroy's \"Next-Task\" and \"Tracer Review\" workflows...\n\n## Status Flow\nbacklog â†’ ready â†’ in_progress â†’ ai_review â†’ human_review â†’ done\n\n## Environment Support\n| Environment | Telemetry | Enforcement |\n|-------------|-----------|-------------|\n| Claude Code | Full      | Hooks       |\n| Cursor      | Full      | Hooks       |\n| OpenCode    | Full      | Plugin      |\n| VS Code     | MCP only  | Instructions|\n\n## Skills\n- /next-task - Select next ticket\n- /review-ticket - Run AI review\n- /demo - Generate demo script\n...\n\n## Installation\nRun `./scripts/install.sh` to configure all environments...\n\n## Troubleshooting\nRun `brain-dump doctor` to diagnose issues...\n```\n\n### docs/environments/ (New Directory)\nPer-environment setup guides:\n- `claude-code.md`\n- `cursor.md`\n- `opencode.md`\n- `vscode.md`\n\n## Acceptance Criteria\n- [ ] CLAUDE.md updated with workflow section\n- [ ] README.md updated with features\n- [ ] docs/universal-workflow.md created\n- [ ] Environment-specific docs created\n- [ ] All documentation references spec\n- [ ] Installation instructions clear\n- [ ] Troubleshooting section added\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (full reference)",
      "priority": "low",
      "tags": [
        "documentation",
        "docs"
      ]
    },
    {
      "id": "eebc19d4-cde5-42ac-82af-317bbcd5a368",
      "title": "Integration test: Full workflow E2E (start â†’ ai_review â†’ human_review â†’ done)",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "Happy path E2E test passes",
        "Review iteration loop test passes",
        "Demo rejection test passes",
        "All precondition enforcement tests pass",
        "Tests run in CI pipeline",
        "Test fixtures don't pollute production database"
      ],
      "references": [
        "Spec: plans/specs/universal-quality-workflow.md (State Machine section)",
        "Existing tests: See e2e/ directory for patterns",
        "mcp-server/tools/__tests__/workflow.test.js"
      ],
      "description": "Create comprehensive end-to-end tests that verify the entire Universal Quality Workflow functions correctly from ticket start through human approval.\n\n## Test Scenarios\n\n### Scenario 1: Happy Path (Full Workflow)\n```\n1. start_ticket_work(ticketId)\n   âœ“ Ticket status changes to in_progress\n   âœ“ Branch created\n   âœ“ Telemetry session started\n   \n2. complete_ticket_work(ticketId, summary)\n   âœ“ Ticket status changes to ai_review\n   âœ“ Workflow state created\n   âœ“ Work summary comment added\n   \n3. submit_review_finding(ticketId, finding)\n   âœ“ Finding stored in database\n   âœ“ findings_count incremented\n   âœ“ Progress comment added\n   \n4. mark_finding_fixed(findingId)\n   âœ“ Finding status updated to 'fixed'\n   âœ“ findings_fixed incremented\n   \n5. check_review_complete(ticketId)\n   âœ“ Returns canProceedToHumanReview: true\n   \n6. generate_demo_script(ticketId, steps)\n   âœ“ Demo script created\n   âœ“ Ticket status changes to human_review\n   âœ“ Workflow state updated (demo_generated: true)\n   \n7. submit_demo_feedback(ticketId, passed: true, feedback)\n   âœ“ Ticket status changes to done\n   âœ“ Demo script marked complete\n   âœ“ Success comment added\n```\n\n### Scenario 2: Review Iteration Loop\n```\n1. Submit findings with critical severity\n2. Attempt generate_demo_script\n   âœ“ Should fail: \"Cannot proceed - open critical findings\"\n3. Mark findings fixed\n4. Retry generate_demo_script\n   âœ“ Should succeed\n```\n\n### Scenario 3: Demo Rejection\n```\n1. Complete through to human_review\n2. submit_demo_feedback(passed: false)\n   âœ“ Ticket stays in human_review (or returns to in_progress)\n   âœ“ Rejection comment added with feedback\n3. AI can pick up and address issues\n```\n\n### Scenario 4: Precondition Enforcement\n```\n- Cannot submit_review_finding if ticket not in ai_review\n- Cannot generate_demo_script if open critical/major findings\n- Cannot submit_demo_feedback if ticket not in human_review\n- Cannot reconcile_learnings if ticket not in done\n```\n\n## Test Implementation\n\n### MCP Tool Tests\nCreate `mcp-server/tools/__tests__/workflow.test.js`:\n- Test each tool in isolation with mock database\n- Test precondition failures return proper error messages\n\n### Integration Tests\nCreate `e2e/workflow.spec.ts`:\n- Full Playwright test running through UI\n- Or direct API tests calling MCP tools in sequence\n\n## Acceptance Criteria\n- [ ] Happy path E2E test passes\n- [ ] Review iteration loop test passes\n- [ ] Demo rejection test passes\n- [ ] All precondition enforcement tests pass\n- [ ] Tests run in CI pipeline\n- [ ] Test fixtures don't pollute production database\n\n## References\n- Spec: plans/specs/universal-quality-workflow.md (State Machine section)\n- Existing tests: See e2e/ directory for patterns",
      "priority": "high",
      "tags": [
        "testing",
        "e2e",
        "integration",
        "workflow"
      ]
    },
    {
      "id": "84c0a4a3-6439-48be-a08a-2cfc8b45af54",
      "title": "Integration test: MCP tool preconditions and error handling",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [
        {
          "decision": "This Matters",
          "alternatives": [],
          "rationale": []
        }
      ],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "All precondition checks implemented in MCP tools",
        "Error messages are helpful and actionable",
        "Tests cover every precondition in table above",
        "Tests verify correct tool is suggested when blocked",
        "Error responses use `isError: true` flag"
      ],
      "references": [
        "MCP Spec: https://spec.modelcontextprotocol.io/specification/server/tools/",
        "Spec: plans/specs/universal-quality-workflow.md (MCP Tools section)"
      ],
      "description": "Verify that all MCP tools properly enforce preconditions and return helpful error messages when preconditions aren't met.\n\n## Why This Matters\n\nMCP tools are the **universal enforcement layer** - they must work correctly regardless of which environment (Claude Code, Cursor, OpenCode, VS Code) is calling them. Precondition checks ensure the workflow can't be bypassed.\n\n## Preconditions to Test\n\n### `start_ticket_work`\n| Precondition | Expected Error |\n|--------------|----------------|\n| Ticket doesn't exist | \"Ticket not found. Use list_tickets to see available.\" |\n| Ticket already in_progress | \"Ticket already in progress.\" |\n| Project has no git repo | \"Project must be a git repository.\" |\n\n### `complete_ticket_work`\n| Precondition | Expected Error |\n|--------------|----------------|\n| Ticket not in in_progress | \"Ticket must be in_progress to complete.\" |\n| No commits on branch | Warning: \"No commits found. Are you sure?\" |\n\n### `submit_review_finding`\n| Precondition | Expected Error |\n|--------------|----------------|\n| Ticket not in ai_review | \"Ticket must be in ai_review status to submit findings.\" |\n| Invalid severity | \"Severity must be one of: critical, major, minor, suggestion\" |\n| Invalid agent | \"Agent must be one of: code-reviewer, silent-failure-hunter, code-simplifier\" |\n\n### `mark_finding_fixed`\n| Precondition | Expected Error |\n|--------------|----------------|\n| Finding doesn't exist | \"Finding not found.\" |\n| Finding already fixed | \"Finding is already marked as fixed.\" |\n\n### `generate_demo_script`\n| Precondition | Expected Error |\n|--------------|----------------|\n| Ticket not in ai_review | \"Ticket must be in ai_review to generate demo.\" |\n| Open critical findings | \"Cannot proceed - X open critical findings must be fixed first.\" |\n| Open major findings | \"Cannot proceed - X open major findings must be fixed first.\" |\n\n### `submit_demo_feedback`\n| Precondition | Expected Error |\n|--------------|----------------|\n| Ticket not in human_review | \"Ticket must be in human_review to submit feedback.\" |\n| No demo script exists | \"No demo script found for this ticket.\" |\n\n### `reconcile_learnings`\n| Precondition | Expected Error |\n|--------------|----------------|\n| Ticket not in done | \"Ticket must be done to reconcile learnings.\" |\n\n## Test Implementation\n\n```javascript\n// mcp-server/tools/__tests__/preconditions.test.js\n\ndescribe('MCP Tool Preconditions', () => {\n  describe('submit_review_finding', () => {\n    it('rejects if ticket not in ai_review', async () => {\n      const result = await callTool('submit_review_finding', {\n        ticketId: 'ticket-in-progress',\n        agent: 'code-reviewer',\n        severity: 'major',\n        description: 'Test finding'\n      });\n      \n      expect(result.isError).toBe(true);\n      expect(result.content[0].text).toContain('must be in ai_review');\n    });\n  });\n});\n```\n\n## Acceptance Criteria\n- [ ] All precondition checks implemented in MCP tools\n- [ ] Error messages are helpful and actionable\n- [ ] Tests cover every precondition in table above\n- [ ] Tests verify correct tool is suggested when blocked\n- [ ] Error responses use `isError: true` flag\n\n## References\n- MCP Spec: https://spec.modelcontextprotocol.io/specification/server/tools/\n- Spec: plans/specs/universal-quality-workflow.md (MCP Tools section)",
      "priority": "high",
      "tags": [
        "testing",
        "mcp",
        "preconditions",
        "error-handling"
      ]
    },
    {
      "id": "414a7ef9-f8d1-4a5c-a0bf-9db2cd75baa3",
      "title": "Integration test: Hook telemetry capture (Claude Code)",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [
        {
          "decision": "This Matters",
          "alternatives": [],
          "rationale": []
        }
      ],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "SessionStart hook creates telemetry session",
        "Stop hook ends telemetry session and flushes queue",
        "PreToolUse captures tool start events",
        "PostToolUse captures tool end events with success",
        "PostToolUseFailure captures failures with error info",
        "Correlation IDs properly pair start/end events",
        "Queue file is properly formatted JSONL",
        "Events appear in database after flush"
      ],
      "references": [
        "Claude Code Hooks: https://code.claude.com/docs/en/hooks",
        "Spec: plans/specs/universal-quality-workflow.md (Claude Code Telemetry section)"
      ],
      "description": "Verify that Claude Code hooks properly capture telemetry events and they flow through to the database.\n\n## Why This Matters\n\nHooks provide **automated telemetry capture** - without them, telemetry would require manual MCP calls. This test ensures the hook â†’ queue â†’ MCP â†’ database pipeline works end-to-end.\n\n## Test Scenarios\n\n### Scenario 1: Session Lifecycle\n```\n1. Start Claude Code session in project with active ticket\n   âœ“ SessionStart hook fires\n   âœ“ Hook outputs message to start telemetry session\n   âœ“ Telemetry session created in database\n\n2. End session (stop/exit)\n   âœ“ Stop hook fires\n   âœ“ Hook outputs message to end telemetry session\n   âœ“ Telemetry session marked complete\n   âœ“ Queue file flushed\n```\n\n### Scenario 2: Tool Event Capture\n```\n1. Start session with telemetry\n2. Execute a Read tool\n   âœ“ PreToolUse hook writes start event to queue\n   âœ“ PostToolUse hook writes end event to queue\n   âœ“ Correlation IDs match between start/end\n\n3. Execute an Edit tool that fails\n   âœ“ PreToolUse hook writes start event\n   âœ“ PostToolUseFailure hook writes end event with error\n   âœ“ success: false in event\n```\n\n### Scenario 3: Queue Processing\n```\n1. Generate 10 tool events into queue\n2. End session\n   âœ“ All events flushed to database\n   âœ“ Queue file cleared\n   âœ“ Events have correct tool names and durations\n```\n\n### Scenario 4: Prompt Capture\n```\n1. Start session with telemetry\n2. Submit user prompt\n   âœ“ UserPromptSubmit hook fires\n   âœ“ Prompt logged (or hash if redacted)\n```\n\n## Test Implementation\n\n### Hook Script Tests\n```bash\n# Test hook scripts directly\n./scripts/test-hooks.sh\n\n# Verify output format\necho '{\"tool_name\": \"Read\"}' | .claude/hooks/log-tool-start.sh\n# Should output JSON event to queue file\n```\n\n### Integration Test\n```typescript\n// e2e/telemetry-hooks.spec.ts\ntest('hooks capture tool events', async () => {\n  // 1. Create test ticket and start work\n  // 2. Simulate tool execution (trigger hooks)\n  // 3. Query telemetry from database\n  // 4. Verify events captured correctly\n});\n```\n\n## Files to Test\n- `.claude/hooks/start-telemetry-session.sh`\n- `.claude/hooks/end-telemetry-session.sh`\n- `.claude/hooks/log-tool-start.sh`\n- `.claude/hooks/log-tool-end.sh`\n- `.claude/hooks/log-tool-failure.sh`\n- `.claude/hooks/log-prompt.sh`\n\n## Acceptance Criteria\n- [ ] SessionStart hook creates telemetry session\n- [ ] Stop hook ends telemetry session and flushes queue\n- [ ] PreToolUse captures tool start events\n- [ ] PostToolUse captures tool end events with success\n- [ ] PostToolUseFailure captures failures with error info\n- [ ] Correlation IDs properly pair start/end events\n- [ ] Queue file is properly formatted JSONL\n- [ ] Events appear in database after flush\n\n## References\n- Claude Code Hooks: https://code.claude.com/docs/en/hooks\n- Spec: plans/specs/universal-quality-workflow.md (Claude Code Telemetry section)",
      "priority": "medium",
      "tags": [
        "testing",
        "hooks",
        "telemetry",
        "claude-code"
      ]
    },
    {
      "id": "d320e4a8-8aee-422c-8b97-1e035974b7a7",
      "title": "Integration test: Cross-environment MCP compatibility",
      "passes": false,
      "overview": "",
      "types": [],
      "designDecisions": [
        {
          "decision": "This Matters",
          "alternatives": [],
          "rationale": []
        }
      ],
      "implementationGuide": [],
      "acceptanceCriteria": [
        "Test each MCP tool from Claude Code",
        "Test each MCP tool from Cursor",
        "Test each MCP tool from OpenCode (via plugin)",
        "Test each MCP tool from VS Code (via MCP extension)",
        "All MCP tools work in Claude Code",
        "All MCP tools work in Cursor",
        "All MCP tools work in OpenCode",
        "All MCP tools work in VS Code",
        "Error responses are consistent across environments",
        "No environment-specific bugs",
        "Concurrent access is handled safely"
      ],
      "references": [
        "MCP Spec: https://spec.modelcontextprotocol.io/",
        "Spec: plans/specs/universal-quality-workflow.md (Environment Feature Matrix)"
      ],
      "description": "Verify that the MCP server works correctly across all supported environments (Claude Code, Cursor, OpenCode, VS Code).\n\n## Why This Matters\n\nThe MCP server is the **universal enforcement layer** - it must behave identically regardless of which AI coding tool is calling it. This test ensures no environment-specific bugs exist.\n\n## Test Matrix\n\n| MCP Tool | Claude Code | Cursor | OpenCode | VS Code |\n|----------|-------------|--------|----------|---------|\n| start_ticket_work | âœ“ | âœ“ | âœ“ | âœ“ |\n| complete_ticket_work | âœ“ | âœ“ | âœ“ | âœ“ |\n| submit_review_finding | âœ“ | âœ“ | âœ“ | âœ“ |\n| mark_finding_fixed | âœ“ | âœ“ | âœ“ | âœ“ |\n| generate_demo_script | âœ“ | âœ“ | âœ“ | âœ“ |\n| submit_demo_feedback | âœ“ | âœ“ | âœ“ | âœ“ |\n| start_telemetry_session | âœ“ | âœ“ | âœ“ | âœ“ |\n| log_tool_event | âœ“ | âœ“ | âœ“ | âœ“ |\n| end_telemetry_session | âœ“ | âœ“ | âœ“ | âœ“ |\n\n## Test Scenarios\n\n### Scenario 1: MCP Transport Compatibility\n```\n1. Call MCP tool via stdio (Claude Code, OpenCode)\n   âœ“ Tool executes successfully\n   âœ“ Response format is correct\n\n2. Call same MCP tool via HTTP/SSE (if applicable)\n   âœ“ Tool executes successfully\n   âœ“ Response format matches stdio\n```\n\n### Scenario 2: Environment Variable Handling\n```\nDifferent environments set different env vars:\n- Claude Code: CLAUDE_PROJECT_DIR\n- Cursor: CURSOR_PROJECT_DIR\n- OpenCode: (via plugin context)\n- VS Code: workspaceFolder variable\n\nâœ“ MCP tools handle missing/different env vars gracefully\nâœ“ Project path detection works in all environments\n```\n\n### Scenario 3: Concurrent Access\n```\n1. Start Claude Code session on ticket\n2. Simultaneously access same ticket from VS Code\n   âœ“ Both see consistent workflow state\n   âœ“ No race conditions on status updates\n   âœ“ Database properly handles concurrent writes\n```\n\n### Scenario 4: Error Response Format\n```\nFor each environment, verify error responses:\nâœ“ Use isError: true flag\nâœ“ Include helpful error message\nâœ“ Suggest correct next action\nâœ“ Format is parseable by AI\n```\n\n## Test Implementation\n\n### MCP Protocol Tests\n```javascript\n// mcp-server/__tests__/protocol.test.js\ndescribe('MCP Protocol Compatibility', () => {\n  it('handles JSON-RPC request format', async () => {\n    const request = {\n      jsonrpc: '2.0',\n      id: 1,\n      method: 'tools/call',\n      params: { name: 'start_ticket_work', arguments: { ticketId: 'test' } }\n    };\n    const response = await handleRequest(request);\n    expect(response.jsonrpc).toBe('2.0');\n    expect(response.id).toBe(1);\n  });\n});\n```\n\n### Manual Testing Checklist\n- [ ] Test each MCP tool from Claude Code\n- [ ] Test each MCP tool from Cursor\n- [ ] Test each MCP tool from OpenCode (via plugin)\n- [ ] Test each MCP tool from VS Code (via MCP extension)\n\n## Acceptance Criteria\n- [ ] All MCP tools work in Claude Code\n- [ ] All MCP tools work in Cursor\n- [ ] All MCP tools work in OpenCode\n- [ ] All MCP tools work in VS Code\n- [ ] Error responses are consistent across environments\n- [ ] No environment-specific bugs\n- [ ] Concurrent access is handled safely\n\n## References\n- MCP Spec: https://spec.modelcontextprotocol.io/\n- Spec: plans/specs/universal-quality-workflow.md (Environment Feature Matrix)",
      "priority": "medium",
      "tags": [
        "testing",
        "mcp",
        "cross-environment",
        "compatibility"
      ]
    }
  ],
  "projectContext": {
    "techStack": [
      "Framework: TanStack Start (React 19 + Vite + Nitro)",
      "Database: SQLite with better-sqlite3, Drizzle ORM",
      "Styling: Tailwind CSS v4",
      "State: TanStack Query for server state",
      "Drag & Drop: @dnd-kit"
    ],
    "dosDonts": [
      {
        "category": "Database Queries",
        "dos": [
          "Use Drizzle ORM: `db.select().from(tickets)`",
          "Use typed schema imports: `import { tickets } from \"../lib/schema\"`",
          "Use `eq()`, `and()`, `sql` from drizzle-orm for conditions",
          "Use transactions for multi-table operations: `db.transaction(() => {...})`",
          "Use `.get()` for single row, `.all()` for multiple"
        ],
        "donts": [
          "Raw SQL strings: `db.run(\"SELECT * FROM tickets\")`",
          "String-based table names",
          "String concatenation for WHERE clauses",
          "Multiple independent queries that should be atomic",
          "Assume query returns what you expect without checking"
        ]
      },
      {
        "category": "React & TanStack Query",
        "dos": [
          "Use `useQuery` with `queryKeys` for data fetching",
          "Use `useMutation` with `onSuccess` for state changes",
          "Invalidate queries after mutations: `queryClient.invalidateQueries({ queryKey: queryKeys.tickets })`",
          "Use centralized `queryKeys` object from `src/lib/hooks.ts`",
          "Create custom hooks in `src/lib/hooks.ts` for reusable query logic"
        ],
        "donts": [
          "`useState` + `useEffect` for fetched data",
          "Direct API calls without proper cache invalidation",
          "Manual cache manipulation or refetch after every change",
          "Hardcoded query key strings throughout components",
          "Duplicate query setup in multiple components"
        ]
      },
      {
        "category": "Styling Patterns",
        "dos": [
          "Use Tailwind classes for static styles: `className=\"p-4 bg-slate-800\"`",
          "Use CSS variables for theming: `var(--bg-primary)`, `var(--shadow-modal)`",
          "Reserve inline styles for dynamic/computed values: `style={{ width: `${percent}%` }}`",
          "Define shared style constants at module level for referential stability",
          "Use `React.CSSProperties` type for inline style objects when needed"
        ],
        "donts": [
          "Inline style objects for single-use styles",
          "Hardcoded colors that don't adapt to themes",
          "Mix Tailwind and inline styles inconsistently",
          "Create new objects inside components that are reused",
          "Untyped style objects that miss IDE autocomplete"
        ]
      },
      {
        "category": "Server Functions",
        "dos": [
          "Use `createServerFn({ method: \"GET\" })` for reads, `\"POST\"` for writes",
          "Use `.inputValidator()` for type-safe input handling",
          "Return typed data directly: `return db.select().from(tickets).all()`",
          "Use `ensureExists()` for required lookups: `ensureExists(project, \"Project\")`",
          "Import from `@tanstack/react-start` (not `@tanstack/react-start/server`)"
        ],
        "donts": [
          "Mix GET/POST inconsistently",
          "Access raw input without validation",
          "Wrap in unnecessary response objects",
          "Return null and let caller handle missing data",
          "Wrong import path"
        ]
      },
      {
        "category": "MCP Tool Implementation",
        "dos": [
          "Use Zod schemas for input validation: `{ ticketId: z.string() }`",
          "Return structured `{ content: [{ type: \"text\", text: ... }] }` format",
          "Set `isError: true` for error responses",
          "Use `log.info()` / `log.error()` from `../lib/logging.js`",
          "Include helpful error messages: `\"Project not found. Use list_projects to see available.\"`"
        ],
        "donts": [
          "Trust input without validation",
          "Return plain strings",
          "Return error text without the error flag",
          "`console.log` in MCP server code",
          "Generic \"Not found\" errors"
        ]
      },
      {
        "category": "Testing Patterns (Kent C. Dodds)",
        "dos": [
          "Test user behavior: what users see, click, and experience",
          "Integration tests for workflows: test components together",
          "Real database fixtures with actual schema",
          "Tests that fail when user-facing behavior breaks",
          "Ask: \"Does this test catch bugs users would encounter?\""
        ],
        "donts": [
          "Test implementation details, internal state, or private methods",
          "Unit tests for every function",
          "Excessive mocking of internals",
          "Tests that break on refactoring internals",
          "Chase 100% code coverage as a goal"
        ]
      }
    ],
    "verificationSteps": [
      "Run `pnpm type-check` - must pass with no errors",
      "Run `pnpm lint` - must pass with no errors",
      "Run `pnpm test` - all tests must pass",
      "Added tests for new functionality (ONLY tests that verify real user behavior - see Testing Philosophy above)",
      "Used typed error classes (not generic `Error`)",
      "Used Drizzle ORM (not raw SQL) - see DO/DON'T table above",
      "Followed existing patterns from DO/DON'T tables",
      "No hardcoded values that should be configurable",
      "Existing tests still pass",
      "No regressions in related functionality",
      "Updated tests if behavior changed",
      "Did not break backward compatibility (unless explicitly requested)",
      "Manually verified in browser at `localhost:4242`",
      "Checked responsive layout",
      "Verified TanStack Query invalidates and updates correctly",
      "Accessibility: keyboard navigation works, proper ARIA labels",
      "Migration file created via `pnpm db:generate`",
      "Migration tested with `pnpm db:migrate`",
      "Backup tested if schema changed (use `pnpm brain-dump backup` then test restore)",
      "Updated `src/lib/schema.ts` with proper types and constraints",
      "Tested tool via Claude Code integration",
      "Verified error responses are informative (see DO/DON'T table)",
      "Updated tool documentation if interface changed",
      "Added Zod schema for input validation",
      "All acceptance criteria from ticket met",
      "Work summary added via `add_ticket_comment` (for Ralph sessions)",
      "Session completed with appropriate outcome (for Ralph sessions)",
      "Committed with proper message format: `feat(<ticket-id>): <description>`"
    ]
  },
  "generatedAt": "2026-01-25T00:48:34.319Z",
  "epicTitle": "Universal Quality Workflow",
  "epicDescription": "Implement Dillon Mulroy's \"Next-Task\" and \"Tracer Review\" workflows across all AI coding environments (Claude Code, Cursor, OpenCode, VS Code). This epic covers the full workflow from ticket selection through human demo approval, with telemetry capture for observability.\n\nSpec: plans/specs/universal-quality-workflow.md\n\nKey deliverables:\n- Database schema for workflow state, review findings, demo scripts\n- MCP tools for workflow transitions and telemetry\n- Environment-specific hooks/plugins (Claude Code, Cursor, OpenCode)\n- Human review UI with demo approval flow\n- Multi-environment install/uninstall scripts"
}